{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b57e21f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/PRTNR/CHUV/DIR/rgottar1/spatial/env/jbac/miniforge3/envs/spatial/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/work/PRTNR/CHUV/DIR/rgottar1/spatial/env/jbac/miniforge3/envs/spatial/lib/python3.13/site-packages/dask/dataframe/__init__.py:31: FutureWarning: The legacy Dask DataFrame implementation is deprecated and will be removed in a future version. Set the configuration option `dataframe.query-planning` to `True` or None to enable the new Dask Dataframe implementation and silence this warning.\n",
      "  warnings.warn(\n",
      "/work/PRTNR/CHUV/DIR/rgottar1/spatial/env/jbac/miniforge3/envs/spatial/lib/python3.13/site-packages/spatialdata/_core/query/relational_query.py:504: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n",
      "  left = partial(_left_join_spatialelement_table)\n",
      "/work/PRTNR/CHUV/DIR/rgottar1/spatial/env/jbac/miniforge3/envs/spatial/lib/python3.13/site-packages/spatialdata/_core/query/relational_query.py:505: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n",
      "  left_exclusive = partial(_left_exclusive_join_spatialelement_table)\n",
      "/work/PRTNR/CHUV/DIR/rgottar1/spatial/env/jbac/miniforge3/envs/spatial/lib/python3.13/site-packages/spatialdata/_core/query/relational_query.py:506: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n",
      "  inner = partial(_inner_join_spatialelement_table)\n",
      "/work/PRTNR/CHUV/DIR/rgottar1/spatial/env/jbac/miniforge3/envs/spatial/lib/python3.13/site-packages/spatialdata/_core/query/relational_query.py:507: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n",
      "  right = partial(_right_join_spatialelement_table)\n",
      "/work/PRTNR/CHUV/DIR/rgottar1/spatial/env/jbac/miniforge3/envs/spatial/lib/python3.13/site-packages/spatialdata/_core/query/relational_query.py:508: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n",
      "  right_exclusive = partial(_right_exclusive_join_spatialelement_table)\n",
      "/work/PRTNR/CHUV/DIR/rgottar1/spatial/env/jbac/miniforge3/envs/spatial/lib/python3.13/site-packages/numba/core/decorators.py:246: RuntimeWarning: nopython is set for njit and is ignored\n",
      "  warnings.warn('nopython is set for njit and is ignored', RuntimeWarning)\n",
      "/work/PRTNR/CHUV/DIR/rgottar1/spatial/env/jbac/miniforge3/envs/spatial/lib/python3.13/site-packages/anndata/utils.py:434: FutureWarning: Importing read_text from `anndata` is deprecated. Import anndata.io.read_text instead.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "\n",
    "dask.config.set({\"dataframe.query-planning\": False})\n",
    "\n",
    "import ot\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from matplotlib.patches import Patch\n",
    "import fastcluster\n",
    "\n",
    "import sys\n",
    "sys.path.extend(['../../scripts','../../scripts/xenium'])\n",
    "import readwrite\n",
    "import preprocessing\n",
    "\n",
    "cfg = readwrite.config()\n",
    "\n",
    "def compute_energy_distance(\n",
    "    adata,\n",
    "    label_key: str,\n",
    "    batch_key: str,\n",
    "    use_rep: str = 'X_pca',\n",
    "    n_subsample = None,\n",
    "    random_state: int = 0\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    (Robust Version) Computes the Energy Distance matrix by building a NumPy\n",
    "    array first and then converting it to a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        adata: The AnnData object, expected to have PCA computed in .obsm.\n",
    "        label_key: The key in adata.obs for cell type labels.\n",
    "        batch_key: The key in adata.obs for batch/patient labels.\n",
    "        use_rep: The representation to use from adata.obsm (e.g., 'X_pca').\n",
    "        n_subsample: If not None, randomly subsamples this many cells from each\n",
    "                     group before calculating the distance.\n",
    "        random_state: Seed for the random subsampling for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        A square pandas DataFrame with a multi-index (cell_type, batch)\n",
    "        containing the pairwise Energy Distances.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    unique_labels = adata.obs[label_key].unique()\n",
    "    unique_batches = adata.obs[batch_key].unique()\n",
    "    combinations = list(product(unique_labels, unique_batches))\n",
    "\n",
    "    combo_data = {}\n",
    "    for label, batch in combinations:\n",
    "        mask = (adata.obs[label_key] == label) & (adata.obs[batch_key] == batch)\n",
    "        n_cells = np.sum(mask)\n",
    "        if n_cells == 0:\n",
    "            continue\n",
    "        if n_subsample is not None and n_cells > n_subsample:\n",
    "            full_data = adata[mask].obsm[use_rep]\n",
    "            indices = rng.choice(n_cells, n_subsample, replace=False)\n",
    "            combo_data[(label, batch)] = full_data[indices, :]\n",
    "        else:\n",
    "            combo_data[(label, batch)] = adata[mask].obsm[use_rep]\n",
    "\n",
    "    combo_names = list(combo_data.keys())\n",
    "    n_combos = len(combo_names)\n",
    "\n",
    "    # --- THE ROBUST PATTERN: Use a NumPy array first ---\n",
    "    dist_array = np.zeros((n_combos, n_combos), dtype=float)\n",
    "\n",
    "    pbar_total = (n_combos**2 - n_combos) // 2\n",
    "    pbar = tqdm(total=pbar_total, desc=\"Calculating Energy Distance\")\n",
    "    for i in range(n_combos):\n",
    "        for j in range(i, n_combos):\n",
    "            if i == j:\n",
    "                continue # Distance is already 0.0\n",
    "            \n",
    "            name_i, name_j = combo_names[i], combo_names[j]\n",
    "            X = combo_data[name_i]\n",
    "            Y = combo_data[name_j]\n",
    "            \n",
    "            between_dist = cdist(X, Y).mean()\n",
    "            within_dist_X = pdist(X).mean() if len(X) > 1 else 0\n",
    "            within_dist_Y = pdist(Y).mean() if len(Y) > 1 else 0\n",
    "            \n",
    "            squared_edist = 2 * between_dist - within_dist_X - within_dist_Y\n",
    "            dist = np.sqrt(max(0, squared_edist))\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Use simple, unambiguous integer indexing on the NumPy array\n",
    "            dist_array[i, j] = dist\n",
    "            dist_array[j, i] = dist\n",
    "            \n",
    "    pbar.close()\n",
    "    \n",
    "    multi_index = pd.MultiIndex.from_tuples(combo_names, names=[label_key, batch_key])\n",
    "    dist_matrix = pd.DataFrame(dist_array, index=multi_index, columns=multi_index)\n",
    "    \n",
    "    return dist_matrix\n",
    "\n",
    "def compute_emd(\n",
    "    adata,\n",
    "    label_key: str,\n",
    "    batch_key: str,\n",
    "    use_rep: str = 'X_pca',\n",
    "    n_subsample = None,\n",
    "    random_state: int = 0\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"  Computes the EMD matrix. \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    unique_labels = adata.obs[label_key].unique()\n",
    "    unique_batches = adata.obs[batch_key].unique()\n",
    "    combinations = list(product(unique_labels, unique_batches))\n",
    "\n",
    "    combo_data = {}\n",
    "    for label, batch in combinations:\n",
    "        mask = (adata.obs[label_key] == label) & (adata.obs[batch_key] == batch)\n",
    "        n_cells = np.sum(mask)\n",
    "        if n_cells == 0:\n",
    "            continue\n",
    "        if n_subsample is not None and n_cells > n_subsample:\n",
    "            full_data = adata[mask].obsm[use_rep]\n",
    "            indices = rng.choice(n_cells, n_subsample, replace=False)\n",
    "            combo_data[(label, batch)] = full_data[indices, :]\n",
    "        else:\n",
    "            combo_data[(label, batch)] = adata[mask].obsm[use_rep]\n",
    "\n",
    "    combo_names = list(combo_data.keys())\n",
    "    n_combos = len(combo_names)\n",
    "\n",
    "    # --- THE ROBUST PATTERN: Use a NumPy array first ---\n",
    "    dist_array = np.zeros((n_combos, n_combos), dtype=float)\n",
    "\n",
    "    pbar = tqdm(total=(n_combos**2 - n_combos) // 2, desc=\"Calculating EMD\")\n",
    "    for i in range(n_combos):\n",
    "        for j in range(i, n_combos):\n",
    "            if i == j:\n",
    "                continue # Distance is already 0\n",
    "            \n",
    "            name_i, name_j = combo_names[i], combo_names[j]\n",
    "            data_i, data_j = combo_data[name_i], combo_data[name_j]\n",
    "            \n",
    "            weights_i = np.ones(len(data_i)) / len(data_i)\n",
    "            weights_j = np.ones(len(data_j)) / len(data_j)\n",
    "            \n",
    "            cost_matrix = ot.dist(data_i, data_j, metric='euclidean')\n",
    "            dist = ot.emd2(weights_i, weights_j, cost_matrix)\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Use integer indices .iloc style on the NumPy array\n",
    "            dist_array[i, j] = dist\n",
    "            dist_array[j, i] = dist\n",
    "            \n",
    "    pbar.close()\n",
    "    \n",
    "    # --- Convert to a DataFrame at the very end ---\n",
    "    multi_index = pd.MultiIndex.from_tuples(combo_names, names=[label_key, batch_key])\n",
    "    dist_matrix = pd.DataFrame(dist_array, index=multi_index, columns=multi_index)\n",
    "    \n",
    "    return dist_matrix\n",
    "\n",
    "\n",
    "\n",
    "def compute_euclidean_distance(\n",
    "    adata,\n",
    "    label_key: str,\n",
    "    batch_key: str,\n",
    "    use_rep: str = 'X_pca',\n",
    "    n_subsample = None,\n",
    "    random_state: int = 0\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes a Euclidean distance matrix between all cells, returning it in the\n",
    "    same multi-index format as the distributional distance functions.\n",
    "\n",
    "    The resulting DataFrame's index and columns will be a MultiIndex based on\n",
    "    the provided label_key and batch_key from adata.obs.\n",
    "\n",
    "    Args:\n",
    "        adata: The AnnData object.\n",
    "        label_key: The key in adata.obs for the first level of the MultiIndex (e.g., 'cell_type').\n",
    "        batch_key: The key in adata.obs for the second level of the MultiIndex (e.g., 'sample').\n",
    "        use_rep: The representation to use from adata.obsm (e.g., 'X_pca').\n",
    "\n",
    "    Returns:\n",
    "        A square pandas DataFrame where the index and columns are a MultiIndex\n",
    "        of (label_key, batch_key) for each cell, and the values are the\n",
    "        pairwise Euclidean distances.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Input Validation ---\n",
    "    if use_rep not in adata.obsm:\n",
    "        raise KeyError(f\"Representation '{use_rep}' not found in adata.obsm. Please compute it first.\")\n",
    "    if label_key not in adata.obs.columns:\n",
    "        raise KeyError(f\"label_key '{label_key}' not found in adata.obs.\")\n",
    "    if batch_key not in adata.obs.columns:\n",
    "        raise KeyError(f\"batch_key '{batch_key}' not found in adata.obs.\")\n",
    "        \n",
    "\n",
    "    # --- 2. Core Distance Calculation (same as before) ---\n",
    "    X = adata.obsm[use_rep]\n",
    "    annot = adata.obs[[label_key, batch_key]]\n",
    "\n",
    "    n_cells = X.shape[0]\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    if n_subsample is not None and n_cells > n_subsample:\n",
    "        indices = rng.choice(n_cells, n_subsample, replace=False)\n",
    "        X = X[indices, :]\n",
    "        annot = annot.iloc[indices]\n",
    "\n",
    "    condensed_distances = pdist(X, metric='euclidean')\n",
    "    distance_matrix_np = squareform(condensed_distances)\n",
    "\n",
    "    # --- 3. Create the MultiIndex for Labeling ---\n",
    "    # This is the key change to match the desired format.\n",
    "    # We create a MultiIndex directly from the columns of the .obs DataFrame.\n",
    "    # The index will have the same length as the number of cells.\n",
    "    multi_index = pd.MultiIndex.from_frame(annot)\n",
    "\n",
    "    # --- 4. Wrap the result in a Labeled DataFrame ---\n",
    "    distance_matrix_df = pd.DataFrame(\n",
    "        distance_matrix_np,\n",
    "        index=multi_index,\n",
    "        columns=multi_index\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return distance_matrix_df\n",
    "\n",
    "def plot_annotated_heatmap(\n",
    "    dist_matrix: pd.DataFrame,\n",
    "    label_palette: dict = None,\n",
    "    batch_palette: dict = None,\n",
    "    linkage_method: str = 'average',\n",
    "    save_path: str = None,\n",
    "    show_label_legend: bool = True,\n",
    "    title = None,\n",
    "    show=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    (From Scratch Legends) Plots a clustered heatmap with manually created and\n",
    "    placed legends for maximum control and clarity.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. Data and Palette Preparation (same as before) ---\n",
    "    label_key, batch_key = dist_matrix.index.names\n",
    "    labels = dist_matrix.index.get_level_values(label_key)\n",
    "    batches = dist_matrix.index.get_level_values(batch_key)\n",
    "    \n",
    "    annot_df = pd.DataFrame({label_key: labels, batch_key: batches}, index=dist_matrix.index)\n",
    "    \n",
    "    if label_palette is None:\n",
    "        unique_labels = annot_df[label_key].unique()\n",
    "        label_palette = dict(zip(unique_labels, sns.color_palette(\"tab10\", len(unique_labels))))\n",
    "    \n",
    "    if batch_palette is None:\n",
    "        unique_batches = annot_df[batch_key].unique()\n",
    "        batch_palette = dict(zip(unique_batches, sns.color_palette(\"Pastel1\", len(unique_batches))))\n",
    "        \n",
    "    row_colors_df = annot_df.copy()\n",
    "    row_colors_df[label_key] = annot_df[label_key].map(label_palette)\n",
    "    row_colors_df[batch_key] = annot_df[batch_key].map(batch_palette)\n",
    "\n",
    "    # linkage\n",
    "    condensed_dist_matrix = squareform(dist_matrix.values)\n",
    "    linkage = fastcluster.linkage(condensed_dist_matrix, method=linkage_method)\n",
    "    \n",
    "    # --- 2. Plot the Clustermap (without worrying about its legends) ---\n",
    "    g = sns.clustermap(\n",
    "        dist_matrix,\n",
    "        cmap='viridis_r',\n",
    "        row_colors=row_colors_df,\n",
    "        col_colors=row_colors_df,\n",
    "        row_linkage=linkage,\n",
    "        col_linkage=linkage,\n",
    "        # linewidths=.5,\n",
    "        figsize=(12, 12),\n",
    "        xticklabels=False,\n",
    "        yticklabels=False,\n",
    "        colors_ratio=0.05,\n",
    "        dendrogram_ratio=(.1, .1)\n",
    "    )\n",
    "    g.ax_heatmap.set_xlabel(\"\")\n",
    "    g.ax_heatmap.set_ylabel(\"\")\n",
    "    # --- 3. Create Legends \"From Scratch\" ---\n",
    "\n",
    "    # Create the graphical \"handles\" for the label legend\n",
    "    if show_label_legend:\n",
    "        label_legend_patches = [\n",
    "            Patch(facecolor=color, edgecolor='black', label=label) \n",
    "            for label, color in label_palette.items() if label in labels\n",
    "        ]\n",
    "        \n",
    "        # Place the first legend on the figure\n",
    "        legend1 = g.fig.legend(\n",
    "            handles=label_legend_patches,\n",
    "            title=label_key,\n",
    "            loc=\"upper right\",\n",
    "            bbox_to_anchor=(1.2, 0.5), # Position: 2% from left, 95% from bottom\n",
    "            fontsize='medium',\n",
    "            frameon=True,\n",
    "            shadow=False,\n",
    "        )\n",
    "\n",
    "    # Create the graphical \"handles\" for the batch legend\n",
    "    batch_legend_patches = [\n",
    "        Patch(facecolor=color, edgecolor='black', label=label) \n",
    "        for label, color in batch_palette.items() if label in batches\n",
    "    ]\n",
    "\n",
    "    # Place the second legend on the figure\n",
    "    legend2 = g.fig.legend(\n",
    "        handles=batch_legend_patches,\n",
    "        title=batch_key,\n",
    "        loc=\"upper right\",\n",
    "        bbox_to_anchor=(1.2, 0.8), # Position: 98% from left, 95% from bottom\n",
    "        fontsize='medium',\n",
    "        frameon=True,\n",
    "        shadow=False,\n",
    "    )\n",
    "    \n",
    "    # --- 4. Final Adjustments ---\n",
    "    \n",
    "    # Set the main title for the entire figure\n",
    "    if title:\n",
    "        g.fig.suptitle(title, y=1.05, fontsize=16, weight='bold')\n",
    "    \n",
    "    if save_path is not None:\n",
    "        p = Path(save_path)\n",
    "        p.parent.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(p, dpi=300, bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ddf5de",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48f3b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "cell_type_annotation_dir = Path(cfg['xenium_cell_type_annotation_dir'])\n",
    "xenium_processed_data_dir = Path(cfg['xenium_processed_data_dir'])\n",
    "xenium_std_seurat_analysis_dir = Path(cfg['xenium_std_seurat_analysis_dir'])\n",
    "normalisation = 'lognorm'\n",
    "layer = 'data'\n",
    "reference = 'matched_reference_combo'\n",
    "method = 'rctd_class_aware'\n",
    "level = 'Level2.1'\n",
    "n_comps = 50\n",
    "max_n_cells = 100_000\n",
    "singlets = False\n",
    "\n",
    "# qc params\n",
    "min_counts = 10\n",
    "min_features = 5\n",
    "max_counts = float(\"inf\")\n",
    "max_features = float(\"inf\")\n",
    "min_cells = 5\n",
    "\n",
    "# common genes and samples to use\n",
    "genes = []\n",
    "samples = []\n",
    "\n",
    "# fixed params\n",
    "OBSM_KEY = \"X_pca\"\n",
    "CT_KEY = 'cell type'#(reference, method, level)\n",
    "BATCH_KEY = \"sample\"\n",
    "annotation_normalisation = \"lognorm\"  # fix this for now, even for sctransfrom\n",
    "\n",
    "palette_dir = Path(cfg['xenium_metadata_dir'])\n",
    "sample_palette = pd.read_csv(palette_dir / 'col_palette_sample.csv').set_index('sample').squeeze()\n",
    "cell_type_palette_path = palette_dir / 'col_palette_cell_types_combo.csv'\n",
    "\n",
    "\n",
    "if level == \"Level2.1\":\n",
    "    palette_lvl2 = (\n",
    "        pd.read_csv(cell_type_palette_path)[[\"Level2\", \"cols_Level2\"]].drop_duplicates().set_index(\"Level2\").squeeze()\n",
    "    )\n",
    "    cell_type_palette = pd.read_csv(cell_type_palette_path)[[level, f\"cols_{level}\"]].drop_duplicates().set_index(level).squeeze()\n",
    "    for k, v in palette_lvl2.items():\n",
    "        if k not in cell_type_palette.index:\n",
    "            cell_type_palette[k] = palette_lvl2[k]\n",
    "\n",
    "else:\n",
    "    cell_type_palette = pd.read_csv(cell_type_palette_path)[[level, f\"cols_{level}\"]].drop_duplicates().set_index(level).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb51f8a",
   "metadata": {},
   "source": [
    "## Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9193df71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing NSCLC 10x_5um chuvio\n",
      "Concatenating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/PRTNR/CHUV/DIR/rgottar1/spatial/env/jbac/miniforge3/envs/spatial/lib/python3.13/site-packages/anndata/_core/anndata.py:1756: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/PRTNR/CHUV/DIR/rgottar1/spatial/env/jbac/miniforge3/envs/spatial/lib/python3.13/site-packages/scanpy/preprocessing/_pca.py:314: ImplicitModificationWarning: Setting element `.obsm['X_pca']` of view, initializing view as actual.\n",
      "  adata.obsm[key_obsm] = X_pca\n",
      "/work/PRTNR/CHUV/DIR/rgottar1/spatial/env/jbac/miniforge3/envs/spatial/lib/python3.13/site-packages/anndata/_core/anndata.py:1756: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n",
      "/work/PRTNR/CHUV/DIR/rgottar1/spatial/env/jbac/miniforge3/envs/spatial/lib/python3.13/site-packages/anndata/_core/anndata.py:1756: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n",
      "Calculating Energy Distance: 100%|██████████| 4005/4005 [01:40<00:00, 39.76it/s] \n"
     ]
    }
   ],
   "source": [
    "correction_method = 'raw'\n",
    "\n",
    "for condition, segmentation, panel_name in [\n",
    "    ('NSCLC', '10x_5um', 'chuvio'),\n",
    "    # ('NSCLC', '10x_5um', 'lung'),\n",
    "    # ('NSCLC', '10x_5um', '5k'),\n",
    "    # ('breast', '10x_5um', 'breast'), \n",
    "    ]:\n",
    "\n",
    "    panel = xenium_std_seurat_analysis_dir / f\"{segmentation}/{condition}/{panel_name}\"\n",
    "\n",
    "    # read xenium samples\n",
    "    print(f\"Processing {condition} {segmentation} {panel_name}\")\n",
    "    ads = {}\n",
    "    for donor in (donors := panel.iterdir()):\n",
    "        for sample in (samples_ := donor.iterdir()):\n",
    "            if len(samples) and sample.stem not in samples:\n",
    "                continue\n",
    "\n",
    "                print(donor.stem, sample.stem)\n",
    "\n",
    "            if segmentation == \"proseg_expected\":\n",
    "                k = (\"proseg\", condition, panel.stem, donor.stem, sample.stem)\n",
    "                k_annot = (segmentation, condition, panel.stem, donor.stem, sample.stem)\n",
    "\n",
    "                name_sample = \"/\".join(k)\n",
    "                name_annot = \"/\".join(k_annot)\n",
    "                sample_dir = xenium_processed_data_dir / f\"{name_sample}/raw_results\"\n",
    "            else:\n",
    "                k = (segmentation.replace(\"proseg_mode\", \"proseg\"), condition, panel.stem, donor.stem, sample.stem)\n",
    "                k_annot = (segmentation, condition, panel.stem, donor.stem, sample.stem)\n",
    "                name_sample = \"/\".join(k)\n",
    "                name_annot = \"/\".join(k_annot)\n",
    "                sample_dir = xenium_processed_data_dir / f\"{name_sample}/normalised_results/outs\"\n",
    "\n",
    "            sample_normalised_counts_path = sample / f\"{normalisation}/normalised_counts/{layer}.parquet\"\n",
    "            sample_idx_path = sample / f\"{normalisation}/normalised_counts/cells.parquet\"\n",
    "\n",
    "            # read normalised data\n",
    "            X_normalised = pd.read_parquet(sample_normalised_counts_path)\n",
    "            X_normalised.index = pd.read_parquet(sample_idx_path).iloc[:, 0]\n",
    "            X_normalised.columns = X_normalised.columns.str.replace(\".\", \"-\")  # undo seurat renaming\n",
    "\n",
    "            if len(genes):\n",
    "                # load raw data to reapply lower bounds QC filters\n",
    "                ads[k] = readwrite.read_xenium_sample(sample_dir, anndata=True)\n",
    "                if segmentation == \"proseg_expected\":\n",
    "                    ads[k].obs_names = \"proseg-\" + ads[k].obs_names.astype(str)\n",
    "\n",
    "                # filter cells\n",
    "                ads[k] = ads[k][X_normalised.index, X_normalised.columns]\n",
    "                ads[k].layers[\"X_normalised\"] = X_normalised\n",
    "                if layer != \"scale_data\":  # no need to sparsify scale_data which is dense\n",
    "                    ads[k].layers[\"X_normalised\"] = scipy.sparse.csr_matrix(ads[k].layers[\"X_normalised\"])\n",
    "            else:\n",
    "                ads[k] = sc.AnnData(X_normalised)\n",
    "                if layer != \"scale_data\":  # no need to sparsify scale_data which is dense\n",
    "                    ads[k].X = scipy.sparse.csr_matrix(ads[k].X)\n",
    "\n",
    "            # read cell type annotation\n",
    "            sample_annotation_dir = cell_type_annotation_dir / f\"{name_annot}/{annotation_normalisation}/reference_based\"\n",
    "            annot_file = sample_annotation_dir / f\"{reference}/{method}/{level}/single_cell/labels.parquet\"\n",
    "            ads[k].obs[CT_KEY] = pd.read_parquet(annot_file).set_index(\"cell_id\").iloc[:, 0]\n",
    "\n",
    "            if singlets:\n",
    "                # read spot class\n",
    "                spot_class_file = (\n",
    "                    sample_annotation_dir / f\"{reference}/{method}/{level}/single_cell/output/results_df.parquet\"\n",
    "                )\n",
    "\n",
    "                ads[k].obs[\"spot_class\"] = pd.read_parquet(spot_class_file, columns=[\"cell_id\", \"spot_class\"]).set_index(\n",
    "                    \"cell_id\"\n",
    "                )\n",
    "                ads[k] = ads[k][ads[k].obs[\"spot_class\"] == \"singlet\"]\n",
    "\n",
    "\n",
    "    print(\"Concatenating\")\n",
    "    # concatenate\n",
    "    xenium_levels = [\"segmentation\", \"condition\", \"panel\", \"donor\", \"sample\"]\n",
    "    for k in ads.keys():\n",
    "        for i, lvl in enumerate(xenium_levels):\n",
    "            ads[k].obs[lvl] = k[i]\n",
    "    ad_merge = sc.concat(ads)\n",
    "    print(\"Done\")\n",
    "\n",
    "    # subset genes\n",
    "    if len(genes):\n",
    "        print(\"Subsetting\")\n",
    "\n",
    "        genes_found = [\n",
    "            g\n",
    "            for g in ad_merge.var_names\n",
    "            if (g in genes) or (g.replace(\".\", \"-\") in genes)  # possible seurat renaming\n",
    "        ]\n",
    "\n",
    "        print(f\"Found {len(genes_found)} out of {len(genes)} genes.\")\n",
    "        ad_merge = ad_merge[:, genes_found].copy()\n",
    "        # reapply QC to subset of genes\n",
    "        preprocessing.preprocess(\n",
    "            ad_merge,\n",
    "            min_counts=min_counts,\n",
    "            min_genes=min_features,\n",
    "            max_counts=max_counts,\n",
    "            max_genes=max_features,\n",
    "            min_cells=min_cells,\n",
    "            save_raw=False,\n",
    "        )\n",
    "        # replace X\n",
    "        ad_merge.X = ad_merge.layers[\"X_normalised\"]\n",
    "\n",
    "    # remove NaN  annotations\n",
    "    ad_merge = ad_merge[ad_merge.obs[CT_KEY].notna()]\n",
    "\n",
    "    #simplify malignant annot\n",
    "    # if condition in [\"NSCLC\",\"mesothelioma_pilot\"]:\n",
    "    #     name_malignant = \"malignant cell of lung\"\n",
    "    # elif condition == \"breast\":\n",
    "    #     name_malignant = \"malignant cell of breast\"\n",
    "    # else:\n",
    "    #     name_malignant = \"malignant cell\"\n",
    "\n",
    "    # ct_to_replace = ad_merge.obs[CT_KEY][ad_merge.obs[CT_KEY].str.contains(\"malignant cell\")].unique()\n",
    "    # replace_map = dict([[ct, name_malignant] for ct in ct_to_replace])\n",
    "    # ad_merge.obs[CT_KEY] = ad_merge.obs[CT_KEY].replace(replace_map)\n",
    "\n",
    "    # subsample to reasonable size\n",
    "    # if len(ad_merge) > max_n_cells:\n",
    "    #     sc.pp.subsample(ad_merge, n_obs=max_n_cells)\n",
    "\n",
    "    # compute pca\n",
    "    sc.tl.pca(ad_merge, n_comps=n_comps)\n",
    "\n",
    "    D = compute_energy_distance(\n",
    "        ad_merge,\n",
    "        label_key=CT_KEY,\n",
    "        batch_key='sample',\n",
    "        use_rep=\"X_pca\",\n",
    "        n_subsample=1000\n",
    "    )\n",
    "    plot_annotated_heatmap(D,label_palette=cell_type_palette,batch_palette=sample_palette,\n",
    "        save_path=cfg['figures_dir']+f'revision/{correction_method}/{segmentation}/{condition}/{panel.stem}/edistance_heatmap_{level}.png')\n",
    "\n",
    "\n",
    "    D = compute_euclidean_distance(\n",
    "        ad_merge,\n",
    "        label_key=CT_KEY,\n",
    "        batch_key='sample',\n",
    "        use_rep=\"X_pca\",\n",
    "        n_subsample=10000\n",
    "    )\n",
    "    plot_annotated_heatmap(D,label_palette=cell_type_palette,batch_palette=sample_palette,\n",
    "        save_path=cfg['figures_dir']+f'revision/{correction_method}/{segmentation}/{condition}/{panel.stem}/euclidean_distance_heatmap_{level}.png')\n",
    "\n",
    "\n",
    "    for ct in ad_merge.obs[CT_KEY].unique():\n",
    "        if 'malignant' in ct:\n",
    "            continue\n",
    "        D_ct = compute_euclidean_distance(\n",
    "            ad_merge[ad_merge.obs[CT_KEY]==ct],\n",
    "            label_key=CT_KEY,\n",
    "            batch_key='sample',\n",
    "            use_rep=\"X_pca\",\n",
    "            n_subsample=5000,\n",
    "        )\n",
    "        plot_annotated_heatmap(D_ct,label_palette=cell_type_palette,batch_palette=sample_palette,\n",
    "            save_path=cfg['figures_dir']+f'revision/{correction_method}/{segmentation}/{condition}/{panel.stem}/euclidean_heatmap_{ct}_{level}.png',\n",
    "            show_label_legend=False,\n",
    "            title=ct\n",
    "        )\n",
    "\n",
    "\n",
    "    D_ct = compute_euclidean_distance(\n",
    "        ad_merge[ad_merge.obs[CT_KEY].str.contains(\"malignant\")],\n",
    "        label_key=CT_KEY,\n",
    "        batch_key='sample',\n",
    "        use_rep=\"X_pca\",\n",
    "        n_subsample=5000,\n",
    "    )\n",
    "    plot_annotated_heatmap(D_ct,label_palette=cell_type_palette,batch_palette=sample_palette,\n",
    "        save_path=cfg['figures_dir']+f'revision/{correction_method}/{segmentation}/{condition}/{panel.stem}/euclidean_heatmap_malignant_{level}.png',\n",
    "        show_label_legend=False,\n",
    "        title = \"malignant cell\"\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5c6bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
